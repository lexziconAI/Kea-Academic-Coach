<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Kea - Academic Coaching Assistant</title>
  <style>
    * { box-sizing: border-box; margin: 0; padding: 0; }
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
      background: linear-gradient(135deg, #1a472a 0%, #2d5a3d 100%);
      color: #eee;
      min-height: 100vh;
      display: flex;
      flex-direction: column;
      align-items: center;
      padding: 2rem;
    }
    h1 { margin-bottom: 0.5rem; color: #7ed957; }
    .subtitle { color: #aaa; margin-bottom: 2rem; text-align: center; }
    .card {
      background: rgba(255,255,255,0.08);
      border-radius: 16px;
      padding: 2rem;
      width: 100%;
      max-width: 500px;
      margin-bottom: 1rem;
    }
    .status {
      display: flex;
      align-items: center;
      justify-content: center;
      gap: 0.5rem;
      margin-bottom: 1rem;
      font-size: 1.1rem;
    }
    .dot {
      width: 14px;
      height: 14px;
      border-radius: 50%;
      background: #666;
    }
    .dot.idle { background: #7ed957; }
    .dot.listening { background: #f5a623; animation: pulse 1s infinite; }
    .dot.speaking { background: #e74c3c; animation: pulse 0.5s infinite; }
    .dot.processing { background: #3498db; animation: pulse 0.3s infinite; }
    @keyframes pulse {
      0%, 100% { opacity: 1; transform: scale(1); }
      50% { opacity: 0.6; transform: scale(1.1); }
    }
    
    .visualizer {
      display: flex;
      align-items: center;
      justify-content: center;
      gap: 3px;
      height: 60px;
      margin-bottom: 1rem;
    }
    .bar {
      width: 6px;
      background: linear-gradient(180deg, #7ed957, #4a8c2a);
      border-radius: 3px;
      transition: height 0.05s ease;
    }
    
    button {
      background: #7ed957;
      color: #1a472a;
      border: none;
      padding: 1.2rem 2rem;
      border-radius: 12px;
      font-size: 1.1rem;
      font-weight: bold;
      cursor: pointer;
      transition: all 0.2s;
      width: 100%;
      margin-bottom: 0.5rem;
    }
    button:hover:not(:disabled) { background: #9aef73; transform: scale(1.02); }
    button:disabled { background: #444; color: #888; cursor: not-allowed; }
    button.active { background: #e74c3c; color: white; }
    
    .transcript {
      background: rgba(0,0,0,0.3);
      border-radius: 8px;
      padding: 1rem;
      min-height: 200px;
      max-height: 400px;
      overflow-y: auto;
      font-size: 0.95rem;
    }
    .log-entry { margin-bottom: 1rem; line-height: 1.5; }
    .log-entry.user { color: #3498db; border-left: 3px solid #3498db; padding-left: 10px; }
    .log-entry.coach { color: #7ed957; border-left: 3px solid #7ed957; padding-left: 10px; }
    .log-entry.system { color: #666; font-size: 0.8rem; text-align: center; }
    .log-entry.error { color: #e74c3c; }
    .metrics { font-size: 0.7rem; color: #555; margin-top: 0.3rem; }
    .hint { text-align: center; color: #888; font-size: 0.85rem; margin-top: 0.5rem; }
  </style>
</head>
<body>
  <h1>ü•ù Kea</h1>
  <p class="subtitle">Academic Coaching Assistant<br>MAMC01810 Managing for Sustainability</p>

  <div class="card">
    <div class="status">
      <div class="dot" id="statusDot"></div>
      <span id="statusText">Click to Start</span>
    </div>
    
    <div class="visualizer" id="visualizer">
      <!-- Bars generated by JS -->
    </div>
    
    <button id="startBtn">üé§ Start Listening</button>
    <p class="hint" id="hint">Click to enable real-time voice detection</p>
  </div>

  <div class="card">
    <h3 style="margin-bottom: 1rem;">Conversation</h3>
    <div class="transcript" id="transcript">
      <div class="log-entry system">Kia ora! Click Start Listening, then tell me about your sustainability research.</div>
    </div>
  </div>

  <script>
    // DOM Elements
    const statusDot = document.getElementById('statusDot');
    const statusText = document.getElementById('statusText');
    const startBtn = document.getElementById('startBtn');
    const hint = document.getElementById('hint');
    const transcript = document.getElementById('transcript');
    const visualizer = document.getElementById('visualizer');

    // State
    let isActive = false;
    let isProcessing = false;
    let isSpeaking = false;
    let audioContext = null;
    let analyser = null;
    let mediaStream = null;
    let mediaRecorder = null;
    let audioChunks = [];
    
    // VAD settings
    const SILENCE_THRESHOLD = 15;      // Volume threshold (0-255)
    const SPEECH_THRESHOLD = 25;       // Threshold to detect speech start
    const SILENCE_DURATION = 1200;     // ms of silence before sending (1.2s)
    const MIN_SPEECH_DURATION = 300;   // Minimum speech duration to send
    
    let silenceStart = null;
    let speechStart = null;
    let isSpeechDetected = false;

    // Create visualizer bars
    const NUM_BARS = 20;
    for (let i = 0; i < NUM_BARS; i++) {
      const bar = document.createElement('div');
      bar.className = 'bar';
      bar.style.height = '4px';
      visualizer.appendChild(bar);
    }
    const bars = visualizer.querySelectorAll('.bar');

    function log(msg, type = 'system', metrics = null) {
      const entry = document.createElement('div');
      entry.className = `log-entry ${type}`;
      
      if (type === 'user') {
        entry.innerHTML = `<strong>You:</strong> ${msg}`;
      } else if (type === 'coach') {
        entry.innerHTML = `<strong>Kea:</strong> ${msg}`;
      } else {
        entry.textContent = msg;
      }
      
      if (metrics) {
        const m = document.createElement('div');
        m.className = 'metrics';
        const total = metrics.stt + metrics.brain + metrics.tts;
        m.textContent = `‚ö° ${total}ms total (STT: ${metrics.stt}ms ‚Ä¢ Brain: ${metrics.brain}ms ‚Ä¢ TTS: ${metrics.tts}ms)`;
        entry.appendChild(m);
      }
      
      transcript.appendChild(entry);
      transcript.scrollTop = transcript.scrollHeight;
    }

    function setStatus(status, text) {
      statusDot.className = 'dot ' + status;
      statusText.textContent = text || status;
    }

    // Audio visualization
    function updateVisualizer() {
      if (!analyser || !isActive) {
        bars.forEach(bar => bar.style.height = '4px');
        return;
      }
      
      const dataArray = new Uint8Array(analyser.frequencyBinCount);
      analyser.getByteFrequencyData(dataArray);
      
      // Sample frequencies for bars
      const step = Math.floor(dataArray.length / NUM_BARS);
      bars.forEach((bar, i) => {
        const value = dataArray[i * step] || 0;
        const height = Math.max(4, (value / 255) * 50);
        bar.style.height = `${height}px`;
      });
      
      if (isActive) requestAnimationFrame(updateVisualizer);
    }

    // Voice Activity Detection
    function processAudio() {
      if (!analyser || !isActive || isProcessing || isSpeaking) return;
      
      const dataArray = new Uint8Array(analyser.frequencyBinCount);
      analyser.getByteFrequencyData(dataArray);
      
      // Calculate average volume
      const avg = dataArray.reduce((a, b) => a + b, 0) / dataArray.length;
      
      const now = Date.now();
      
      if (avg > SPEECH_THRESHOLD) {
        // Speech detected
        if (!isSpeechDetected) {
          isSpeechDetected = true;
          speechStart = now;
          silenceStart = null;
          setStatus('listening', 'Listening...');
          
          // Start recording if not already
          if (!mediaRecorder || mediaRecorder.state === 'inactive') {
            startRecording();
          }
        }
        silenceStart = null;
      } else if (isSpeechDetected && avg < SILENCE_THRESHOLD) {
        // Silence after speech
        if (!silenceStart) {
          silenceStart = now;
        } else if (now - silenceStart > SILENCE_DURATION) {
          // Enough silence - send the audio
          const speechDuration = now - speechStart;
          if (speechDuration > MIN_SPEECH_DURATION) {
            stopRecordingAndSend();
          } else {
            // Too short, reset
            resetRecording();
          }
          isSpeechDetected = false;
          speechStart = null;
          silenceStart = null;
        }
      }
      
      if (isActive) setTimeout(processAudio, 50);
    }

    function startRecording() {
      if (mediaRecorder && mediaRecorder.state === 'recording') return;
      
      audioChunks = [];
      
      let options = {};
      if (MediaRecorder.isTypeSupported('audio/webm;codecs=opus')) {
        options.mimeType = 'audio/webm;codecs=opus';
      }
      
      mediaRecorder = new MediaRecorder(mediaStream, options);
      
      mediaRecorder.ondataavailable = (e) => {
        if (e.data.size > 0) audioChunks.push(e.data);
      };
      
      mediaRecorder.start(100); // Collect chunks every 100ms
    }

    function resetRecording() {
      if (mediaRecorder && mediaRecorder.state === 'recording') {
        mediaRecorder.stop();
      }
      audioChunks = [];
      setStatus('idle', 'Listening... speak anytime');
    }

    async function stopRecordingAndSend() {
      if (!mediaRecorder || mediaRecorder.state !== 'recording') return;
      
      setStatus('processing', 'Processing...');
      isProcessing = true;
      
      // Stop recording and wait for data
      await new Promise(resolve => {
        mediaRecorder.onstop = resolve;
        mediaRecorder.stop();
      });
      
      if (audioChunks.length === 0) {
        isProcessing = false;
        setStatus('idle', 'Listening... speak anytime');
        return;
      }
      
      const blob = new Blob(audioChunks, { type: mediaRecorder.mimeType || 'audio/webm' });
      audioChunks = [];
      
      await sendToCoach(blob);
    }

    async function sendToCoach(audioBlob) {
      try {
        const base64 = await new Promise((resolve, reject) => {
          const reader = new FileReader();
          reader.onloadend = () => resolve(reader.result.split(',')[1]);
          reader.onerror = reject;
          reader.readAsDataURL(audioBlob);
        });

        const response = await fetch('/api/voice', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({ audio: base64 })
        });

        const result = await response.json();

        if (!result.success) {
          throw new Error(result.error || 'Unknown error');
        }

        // Show transcript
        log(result.transcript, 'user');
        log(result.response, 'coach', result.metrics);

        // Play audio response
        if (result.audio) {
          isSpeaking = true;
          setStatus('speaking', 'Kea is speaking...');
          
          const audio = new Audio(`data:${result.mimeType || 'audio/mp3'};base64,${result.audio}`);
          
          audio.onended = () => {
            console.log('[KEA] Audio playback ended, resuming listening');
            isSpeaking = false;
            isProcessing = false;
            setStatus('idle', 'Listening... speak anytime');
            // CRITICAL: Restart the VAD loop
            if (isActive) {
              setTimeout(processAudio, 100);
            }
          };
          
          audio.onerror = (e) => {
            console.error('[KEA] Audio error:', e);
            isSpeaking = false;
            isProcessing = false;
            setStatus('idle', 'Listening... speak anytime');
            if (isActive) {
              setTimeout(processAudio, 100);
            }
          };
          
          await audio.play();
        } else {
          isProcessing = false;
          setStatus('idle', 'Listening... speak anytime');
          // Restart VAD loop
          if (isActive) {
            setTimeout(processAudio, 100);
          }
        }

      } catch (err) {
        log(`Error: ${err.message}`, 'error');
        isProcessing = false;
        isSpeaking = false;
        setStatus('idle', 'Listening... speak anytime');
        // Restart VAD loop even on error
        if (isActive) {
          setTimeout(processAudio, 100);
        }
      }
    }

    async function startListening() {
      try {
        mediaStream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
          }
        });
        
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        analyser = audioContext.createAnalyser();
        analyser.fftSize = 256;
        analyser.smoothingTimeConstant = 0.8;
        
        const source = audioContext.createMediaStreamSource(mediaStream);
        source.connect(analyser);
        
        isActive = true;
        setStatus('idle', 'Listening... speak anytime');
        startBtn.textContent = '‚èπÔ∏è Stop Listening';
        startBtn.classList.add('active');
        hint.textContent = 'Speak naturally ‚Ä¢ I detect when you pause';
        
        log('Real-time listening active. Just speak!', 'system');
        
        updateVisualizer();
        processAudio();
        
      } catch (err) {
        log(`Microphone error: ${err.message}`, 'error');
      }
    }

    function stopListening() {
      isActive = false;
      
      if (mediaRecorder && mediaRecorder.state === 'recording') {
        mediaRecorder.stop();
      }
      
      if (mediaStream) {
        mediaStream.getTracks().forEach(t => t.stop());
        mediaStream = null;
      }
      
      if (audioContext) {
        audioContext.close();
        audioContext = null;
      }
      
      analyser = null;
      audioChunks = [];
      isSpeechDetected = false;
      
      setStatus('', 'Click to Start');
      startBtn.textContent = 'üé§ Start Listening';
      startBtn.classList.remove('active');
      hint.textContent = 'Click to enable real-time voice detection';
      
      bars.forEach(bar => bar.style.height = '4px');
    }

    // Toggle listening
    startBtn.onclick = () => {
      if (isActive) {
        stopListening();
      } else {
        startListening();
      }
    };
  </script>
</body>
</html>
